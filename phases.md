üß† Project Overview for Cursor
üéØ Goal: Full Sign Language Recognition System
The goal of this project is to build an end-to-end Sign Language Recognition system that can accurately translate video-based American Sign Language (ASL) into text or speech, using deep learning and computer vision.

üß± Project Phases & Current Progress
‚úÖ 1. Static ASL Alphabet Classification
Dataset: ASL Alphabet images.

Built and trained a VGG-style CNN model.

Achieved 99%+ accuracy on validation/test sets.

Integrated with webcam: live letter prediction.

‚û°Ô∏è Completed.

‚úÖ 2. Word-Level Sign Recognition (Current Phase)
Dataset: WLASL (Word-Level ASL) ‚Äî videos of isolated glosses (ASL words).

Steps completed so far:

‚úÖ Downloaded and partially filtered WLASL videos (over 3,900 processed).

‚úÖ Generated a labels.csv file mapping video filenames to gloss labels.

‚úÖ Identified 825 unique gloss labels.

üõ† Working on: video_to_frames.py ‚Äî script to convert each .mp4 video into a sequence of uniformly sampled and resized frames, saved as .npy.

‚û°Ô∏è This script has not successfully run yet ‚Äî needs bugfixes due to corrupted videos and timeout issues.

‚è≥ 3. Temporal Modeling with LSTM / GRU
Use the CNN frame encoder on videos.

Extract frame-wise embeddings.

Train an LSTM/GRU/Transformer model for word classification based on temporal input.

Optionally use CTC loss for variable-length glosses.

‚è≥ 4. Real-Time Word Prediction Pipeline
Webcam feed processed frame-by-frame.

Frame buffer passed to temporal model.

Continuous prediction of signs (real-time gloss recognition).

‚è≥ 5. UI / Deployment
Build a minimal interface showing:

Webcam feed.

Live predicted gloss/letter.

Optionally generate sentence using language model.

Optionally output text-to-speech.

Export to TFLite / ONNX for lightweight deployment.